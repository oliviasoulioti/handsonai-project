# -*- coding: utf-8 -*-
"""Hands-on AI - Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ENE9Bcx9bVWNkXusY1jg7Dm3y4ovNNy
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""#1. Φόρτωση Συνόλου Δεδομένων"""

# URL του dataset (σε .xls μορφή)
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls'

# Φόρτωση του αρχείου .xls (το πρώτο φύλλο του Excel)
df = pd.read_excel(url, header=1)

# Εμφάνιση των πρώτων γραμμών
df.head()

"""#2. Προεπεξεργασία Δεδομένων

## 2.1 Αντιμετώπιση Ελλιπών Τιμών (Missing Values)
"""

missing = df.isnull().sum()
missing[missing > 0]

"""## 2.2 Αντιμετώπιση Ακραίων Τιμών και Ανωμαλιών (Outliers & Anomalies)

Οπτικοποιούμε τις πιο χαρακτηριστικές στήλες.
"""

cols = ['LIMIT_BAL', 'AGE'] + [f'BILL_AMT{i}' for i in range(1, 7)] + [f'PAY_AMT{i}' for i in range(1, 7)]

# Ρυθμίσεις plot
fig, axes = plt.subplots(nrows=2, ncols=7, figsize=(20, 6))
axes = axes.flatten()

for i, col in enumerate(cols):
    sns.boxplot(data=df, x=col, ax=axes[i], color='lightblue')
    axes[i].set_title(col)
    axes[i].set_xlabel('')
    axes[i].tick_params(axis='x', labelrotation=45)

plt.tight_layout()
plt.show()

"""Θα εφαρμόσουμε log-transformation σε αυτά τα features για να μειώσουμε την επίδραση των outliers. Αυτό δεν αντικαθιστά τις αρχικές στήλες αλλά δημιουργεί νέες π.χ. LIMIT_BAL_log, τις οποίες μπορούμε να επιλέξουμε αργότερα για μοντελοποίηση."""

cols_to_log = ['LIMIT_BAL'] + \
              [f'BILL_AMT{i}' for i in range(1, 7)] + \
              [f'PAY_AMT{i}' for i in range(1, 7)]

for col in cols_to_log:
    new_col = f'{col}_log'
    df[new_col] = df[col].apply(lambda x: np.log1p(x) if x > 0 else 0)

"""## 2.3 Κανονικοποίηση / Τυποποίηση

Επιλέγουμε μόνο αριθμητικές στήλες.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Αριθμητικά πεδία προς τυποποίηση
scale_cols = ['AGE']
df[scale_cols] = scaler.fit_transform(df[scale_cols])

# Επιλογή όλων των log-transformed στηλών
log_cols = [col for col in df.columns if col.endswith('_log')]
df[log_cols] = scaler.fit_transform(df[log_cols])

"""## 2.4 Feature Engineering

Προτεινόμενα νέα χαρακτηριστικά:
1. Μέση κατάσταση καθυστέρησης
2. Συνολικό απλήρωτο υπόλοιπο
3. Λόγος πληρωμών προς χρέωση
"""

pay_cols = [f'PAY_{i}' for i in [0, 2, 3, 4, 5, 6]]
df['avg_delay'] = df[pay_cols].mean(axis=1)

bill_cols = [f'BILL_AMT{i}' for i in range(1, 7)]
df['total_bill'] = df[bill_cols].sum(axis=1)

pay_cols = [f'PAY_AMT{i}' for i in range(1, 7)]
df['total_payment'] = df[pay_cols].sum(axis=1)
df['pay_to_bill_ratio'] = df['total_payment'] / (df['total_bill'] + 1)  # +1 για αποφυγή διαίρεσης με 0

"""## 2.5 Μετατροπή Κατηγορικών Μεταβλητών (Categorical Encoding)"""

cat_cols = ['SEX', 'EDUCATION', 'MARRIAGE'] + [f'PAY_{i}' for i in [0, 2, 3, 4, 5, 6]]

# One-hot encoding (αυτόματα drop first για αποφυγή πολυγραμμικότητας)
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

df.head()

"""## 2.6 Επιλογή Χαρακτηριστικών (Feature Selection)"""

# Target μεταβλητή
target = 'default payment next month'

# Drop ID & target από τα χαρακτηριστικά
X = df.drop(columns=['ID', target])
y = df[target]

# Μέθοδος 1: SelectKBest (Univariate Feature Selection)
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=20)
X_selected = selector.fit_transform(X, y)

# Εμφάνιση των σημαντικότερων χαρακτηριστικών
selected_mask = selector.get_support()
selected_features_kbest = X.columns[selected_mask]
print("SelectKBest:", selected_features_kbest.tolist())

# Μέθοδος 2: Tree-based Feature Importance (Random Forest)
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)
model.fit(X, y)

importances = model.feature_importances_
indices = importances.argsort()[::-1]

# Top 20 χαρακτηριστικά
selected_features_tree = X.columns[indices[:20]]
print("Tree-based:", selected_features_tree.tolist())

# Ένωση δύο sets και μετατροπή πίσω σε λίστα
combined_features = list(set(selected_features_kbest) | set(selected_features_tree))

print(f"Συνολικά χαρακτηριστικά μετά την ένωση: {len(combined_features)}")
print("Στήλες:\n", combined_features)

common_features = list(set(selected_features_kbest) & set(selected_features_tree))
print("Κοινές στήλες:\n", common_features)

X_combined = df[combined_features]

X_common = df[common_features]

"""## 2.7 Μείωση Διάστασης με PCA"""

from sklearn.decomposition import PCA

# Δημιουργία PCA με στόχο να διατηρηθεί 95% της διακύμανσης
pca = PCA(n_components=0.95, random_state=42)

# Εφαρμογή PCA σε όλα τα features που επιλέξαμε
X_pca = pca.fit_transform(df[combined_features])

print(f"Αρχικά χαρακτηριστικά: {len(combined_features)}")
print(f"Νέα χαρακτηριστικά μετά το PCA: {X_pca.shape[1]}")

"""# 3. Επιλογή, Εκπαίδευση και Βελτιστοποίηση Μοντέλου

Μετά από την προεπεξεργασία, έχουμε διαφορετικές εκδοχές του συνόλου δεδομένων που μπορούμε να χρησιμοποιήσουμε για εκπαίδευση:

1. Full: Το πλήρες dataset με 99 χαρακτηριστικά.
2. Combined: Τα 34 χαρακτηριστικά που προέκυψαν από 2 μεθόδους επιλογής χαρακτηριστικών.
3. Common: Τα κοινά 6 χαρακτηριστικά από τις 2 μεθόδους επιλογής χαρακτηριστικών.
4. PCA: Τα 2 χαρακτηριστικά που προέκυψαν από PCA στα combined χαρακτηριστικά.
"""

from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, test_size=0.2, stratify=y, random_state=42
# )

X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y, test_size=0.2, stratify=y, random_state=42
)

# X_train, X_test, y_train, y_test = train_test_split(
#     X_common, y, test_size=0.2, stratify=y, random_state=42
# )

# X_train, X_test, y_train, y_test = train_test_split(
#     X_pca, y, test_size=0.2, stratify=y, random_state=42
# )

"""## 3.1 Μοντέλο 1 – Random Forest (ML)"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
print(classification_report(y_test, y_pred))

"""## 3.2 Μοντέλο 2 – MLP (Deep Learning)"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[callback])

y_pred_probs = model.predict(X_test).ravel()
y_pred = (y_pred_probs > 0.5).astype(int)

print(classification_report(y_test, y_pred))

"""Δοκιμάζουμε και τις 4 εκδοχές και έχουμε τα παρακάτω αποτελέσματα σχετικά με την ακρίβεια:

|                | FUll | Combined | Common | PCA  |
|----------------|------|----------|--------|------|
| Random Forest  | 0.81 | 0.81     | 0.78   | 0.74 |
| MLP            | 0.78 | 0.78     | 0.75   | 0.60 |

Επομένως, συνεχίζουμε στο hyperparameter tuning με το combined dataset (34 χαρακτηριστικά).

# 3.3 Tuning για Random Forest με Optuna
"""

!pip install optuna

import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score, make_scorer

def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 50, 300)
    max_depth = trial.suggest_int('max_depth', 4, 20)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)
    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])

    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        random_state=42,
        n_jobs=-1
    )

    score = cross_val_score(
        clf, X_train, y_train,
        scoring=make_scorer(f1_score),
        cv=3,
        n_jobs=-1
    )
    return score.mean()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30)

print("Καλύτερες υπερπαράμετροι:")
print(study.best_params)

best_params = study.best_params

best_rf = RandomForestClassifier(**best_params, random_state=42)
best_rf.fit(X_train, y_train)
y_pred = best_rf.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

y_prob = best_rf.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_prob)
auc_score = roc_auc_score(y_test, y_prob)

plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

"""## 3.4 Tuning για MLP με KerasTuner"""

!pip install keras-tuner --quiet

import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy

def build_model(hp):
    model = Sequential()
    model.add(Dense(
        hp.Int('units_1', min_value=32, max_value=256, step=32),
        activation='relu',
        input_shape=(X_train.shape[1],)
    ))
    model.add(Dropout(hp.Float('dropout_1', 0.0, 0.5, step=0.1)))

    model.add(Dense(
        hp.Int('units_2', min_value=16, max_value=128, step=16),
        activation='relu'
    ))
    model.add(Dropout(hp.Float('dropout_2', 0.0, 0.5, step=0.1)))

    model.add(Dense(1, activation='sigmoid'))

    model.compile(
        optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss=BinaryCrossentropy(),
        metrics=['accuracy']
    )
    return model

tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=30,
    executions_per_trial=1,
    directory='mlp_tuning',
    project_name='credit_default'
)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

tuner.search(X_train, y_train, epochs=100, validation_split=0.2, verbose=1, callbacks=[callback])

best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]
model = tuner.hypermodel.build(best_hp)

history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=64, callbacks=[callback])

from sklearn.metrics import classification_report, confusion_matrix

y_pred_probs = model.predict(X_test).ravel()
y_pred = (y_pred_probs > 0.5).astype(int)

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()

"""Τα αποτελέσματα είναι:

|                | Before tuning | After tuning |
|----------------|---------------|--------------|
| Random Forest  | 0.81          | 0.82   |
| MLP            | 0.78          | 0.73   |

Άρα, καλύτερο είναι το μοντέλο Random Forest για το πρόβλημά μας.

# 4. Ερμηνευσιμότητα Μοντέλου Random Forest
"""

from sklearn.inspection import permutation_importance

result = permutation_importance(best_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

# Ταξινόμηση σημαντικότητας
importances = result.importances_mean
indices = importances.argsort()[::-1]

# Top 15 χαρακτηριστικά
top_features = X.columns[indices[:15]]

plt.figure(figsize=(8, 5))
plt.barh(top_features[::-1], importances[indices[:15]][::-1])
plt.title("Permutation Feature Importance (Random Forest)")
plt.xlabel("Importance")
plt.grid(True)
plt.tight_layout()
plt.show()

